{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NOTE\n",
        "\n",
        "For Spark 3 Cosmos DB connector has slightly different configuration. Please select different snippest if a Spark 3.1 + pool is attached."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-10-29T17:54:54.8055721Z",
              "execution_start_time": "2022-10-29T17:54:54.6016584Z",
              "livy_statement_state": "available",
              "queued_time": "2022-10-29T17:54:54.4810621Z",
              "session_id": "0",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "thepool",
              "state": "finished",
              "statement_id": 4
            },
            "text/plain": [
              "StatementMeta(thepool, 0, 4, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Create databases and containers\n",
        "\n",
        "cosmosEndpoint = \"https://<your-db>.documents.azure.com:443/\"\n",
        "cosmosMasterKey = \"<your key>\"\n",
        "cosmosDatabaseName = \"sampleDB\"\n",
        "cosmosContainerName = \"sampleContainer\"\n",
        "\n",
        "cfg = {\n",
        "  \"spark.cosmos.accountEndpoint\" : cosmosEndpoint,\n",
        "  \"spark.cosmos.accountKey\" : cosmosMasterKey,\n",
        "  \"spark.cosmos.database\" : cosmosDatabaseName,\n",
        "  \"spark.cosmos.container\" : cosmosContainerName,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-10-29T17:55:36.3291162Z",
              "execution_start_time": "2022-10-29T17:55:35.2302958Z",
              "livy_statement_state": "available",
              "queued_time": "2022-10-29T17:55:35.0932337Z",
              "session_id": "0",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "thepool",
              "state": "finished",
              "statement_id": 6
            },
            "text/plain": [
              "StatementMeta(thepool, 0, 6, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Configure Catalog Api to be used\n",
        "spark.conf.set(\"spark.sql.catalog.cosmosCatalog\", \"com.azure.cosmos.spark.CosmosCatalog\")\n",
        "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.accountEndpoint\", cosmosEndpoint)\n",
        "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.accountKey\", cosmosMasterKey)\n",
        "\n",
        "# create an Azure Cosmos DB database using catalog api\n",
        "spark.sql(\"CREATE DATABASE IF NOT EXISTS cosmosCatalog.{};\".format(cosmosDatabaseName))\n",
        "\n",
        "# create an Azure Cosmos DB container using catalog api\n",
        "spark.sql(\"CREATE TABLE IF NOT EXISTS cosmosCatalog.{}.{} using cosmos.oltp TBLPROPERTIES(partitionKeyPath = '/id', manualThroughput = '400')\".format(cosmosDatabaseName, cosmosContainerName))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-10-29T17:56:02.8554294Z",
              "execution_start_time": "2022-10-29T17:55:45.5074617Z",
              "livy_statement_state": "available",
              "queued_time": "2022-10-29T17:55:45.3464505Z",
              "session_id": "0",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "thepool",
              "state": "finished",
              "statement_id": 7
            },
            "text/plain": [
              "StatementMeta(thepool, 0, 7, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Ingest data\n",
        "\n",
        "spark.createDataFrame(((\"cat-alive\", \"Schrodinger cat\", 2, True), (\"cat-dead\", \"Schrodinger cat\", 2, False)))\\\n",
        "  .toDF(\"id\",\"name\",\"age\",\"isAlive\") \\\n",
        "   .write\\\n",
        "   .format(\"cosmos.oltp\")\\\n",
        "   .options(**cfg)\\\n",
        "   .mode(\"APPEND\")\\\n",
        "   .save()   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-10-29T17:57:42.0071559Z",
              "execution_start_time": "2022-10-29T17:57:33.2767927Z",
              "livy_statement_state": "available",
              "queued_time": "2022-10-29T17:57:33.1485245Z",
              "session_id": "0",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "thepool",
              "state": "finished",
              "statement_id": 8
            },
            "text/plain": [
              "StatementMeta(thepool, 0, 8, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+---------------+---+-------+\n",
            "|       id|           name|age|isAlive|\n",
            "+---------+---------------+---+-------+\n",
            "|cat-alive|Schrodinger cat|  2|   true|\n",
            "+---------+---------------+---+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Query data\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df = spark.read.format(\"cosmos.oltp\").options(**cfg)\\\n",
        " .option(\"spark.cosmos.read.inferSchema.enabled\", \"true\")\\\n",
        " .load()\n",
        "\n",
        "df.filter(col(\"isAlive\") == True)\\\n",
        " .show()"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
